<!--
 Copyright 2025 Umang Patel
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
 
     https://www.apache.org/licenses/LICENSE-2.0
 
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

# Distributed File System - Complete Technical Report

**Project**: GFS-like Distributed File System with Exactly-Once Append Semantics  
**Author**: Umang Patel  
**Course**: Distributed Systems (Semester 7)  
**Date**: December 2025

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Architecture](#system-architecture)
3. [Core Components](#core-components)
4. [Operations and Workflows](#operations-and-workflows)
5. [Metadata Management](#metadata-management)
6. [Exactly-Once Append Semantics](#exactly-once-append-semantics)
7. [Replication and Consistency](#replication-and-consistency)
8. [Failure Handling and Recovery](#failure-handling-and-recovery)
9. [Performance Optimizations](#performance-optimizations)
10. [Testing and Validation](#testing-and-validation)
11. [Benchmarking Results](#benchmarking-results)
12. [Limitations and Future Work](#limitations-and-future-work)

---

## Executive Summary

This project implements a distributed file system inspired by the Google File System (GFS) architecture, with significant enhancements to provide **exactly-once append semantics**. The system is built in Go and uses gRPC for inter-component communication.

### Key Features

- **64MB chunk-based storage** with 3-way replication
- **Exactly-once append guarantees** using modified two-phase commit with idempotency tracking
- **Primary-secondary replication** with lease-based coordination
- **Write-Ahead Logging (WAL)** for master crash recovery
- **Heartbeat-based failure detection** and monitoring
- **Garbage collection** for orphaned chunks
- **Persistent state management** for chunk servers
- **Comprehensive benchmarking suite** for latency and throughput analysis

### Technology Stack

- **Language**: Go 1.23
- **RPC Framework**: gRPC with Protocol Buffers
- **Configuration**: YAML-based
- **Build System**: Makefile
- **Testing**: Go testing framework with custom test scenarios

---

## System Architecture

### High-Level Overview

The system follows a master-worker architecture with three primary components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Application                    â”‚
â”‚         (CLI, Interactive REPL, Benchmarks)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ 1. Get Metadata
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ 2. Lease Info
         â”‚  Master Server   â”‚â—„â”€â”€â”€â”˜
         â”‚  (Metadata)      â”‚
         â”‚  - Fileâ†’Chunks   â”‚
         â”‚  - Chunkâ†’Locationsâ”‚
         â”‚  - Leases        â”‚
         â”‚  - WAL           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ 3. Heartbeats
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ChunkSvrâ”‚   â”‚ChunkSvrâ”‚   â”‚ChunkSvrâ”‚   â”‚ChunkSvrâ”‚
â”‚  :6000 â”‚   â”‚  :6001 â”‚   â”‚  :6002 â”‚   â”‚  :6003 â”‚
â”‚        â”‚   â”‚        â”‚   â”‚        â”‚   â”‚        â”‚
â”‚ Primaryâ”‚   â”‚Secondary   â”‚Secondary   â”‚Secondary
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²            â–²            â–²
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          4. Data Pipeline
          5. Replication
```

### Component Distribution

1. **Master Server** (`:5000`)
   - Single point of metadata coordination
   - Does not handle data operations
   - Manages chunk location and lease information

2. **ChunkServers** (`:6000-6003`)
   - Store actual file data as 64MB chunks
   - Handle read/write/append operations
   - Replicate data to maintain 3 copies
   - Report health and chunk inventory via heartbeats

3. **Clients**
   - Command-line interface ([`cmd/client`](cmd/client/main.go))
   - Interactive REPL ([`cmd/interactive`](cmd/interactive/main.go))
   - Benchmark utilities ([`benchmarks/`](benchmarks/))

---

## Core Components

### 1. Master Server

**Location**: [`internal/master/master.go`](internal/master/master.go)

The master is the brain of the system, managing all metadata and coordinating operations.

#### Data Structures

```go
type MasterServer struct {
    meta   *MasterMetadata      // File and chunk metadata
    log    *OperationLog        // Write-ahead log
    leases *LeaseManager        // Lease coordination
    
    replicationFactor int       // Default: 3
    leaseDuration     time.Duration // Default: 60s
    
    commandQueue map[string][]*HeartBeatCommand // Pending commands
    garbage      map[string]time.Time            // GC candidates
}

type MasterMetadata struct {
    FileMap        map[string][]string           // filename â†’ chunk handles
    ChunkMap       map[string]*ChunkInfo         // handle â†’ chunk metadata
    ChunkLocations map[string]map[string]struct{} // handle â†’ replica addresses
    Replicas       map[string]*ReplicaState      // address â†’ server state
    StaleReplicas  map[string]map[string]struct{} // handle â†’ stale addresses
}
```

#### Key Responsibilities

1. **Namespace Management**: Maintains file-to-chunk mappings
2. **Chunk Location Tracking**: Knows which servers have which chunks
3. **Lease Management**: Grants primary leases for write coordination
4. **Failure Detection**: Monitors chunk server health via heartbeats
5. **Replication Coordination**: Ensures 3-way replication
6. **Garbage Collection**: Cleans up orphaned chunks

### 2. ChunkServer

**Location**: [`internal/chunkserver/chunkserver.go`](internal/chunkserver/chunkserver.go)

ChunkServers are the workhorses that store and serve actual file data.

#### Data Structures

```go
type ChunkServer struct {
    addr         string                    // Server address
    masterAddr   string                    // Master address
    dataDir      string                    // Local storage path
    
    // Chunk metadata
    chunkSizes   map[string]int64          // handle â†’ size
    chunkVersion map[string]int64          // handle â†’ version
    
    // Append state tracking (for exactly-once)
    appendStates map[string]map[string]*appendRecordState
    
    // Buffered data (for pipelined writes)
    dataBuffer   map[string][]byte         // dataID â†’ payload
    
    // Leases held as primary
    primaryLeases map[string]time.Time     // handle â†’ expiration
    
    // Persistent state
    statePath    string
    stateDirty   atomic.Bool
}

type appendRecordState struct {
    offset           int64
    length           int64
    originalSize     int64
    status           appendStatus    // preparing, prepared, committed
    committedVersion int64
    lastUpdated      time.Time
}
```

#### Key Responsibilities

1. **Data Storage**: Persists chunks as files (`chunk_<handle>.chk`)
2. **Version Control**: Tracks chunk versions for consistency
3. **Buffer Management**: LRU cache for uncommitted data
4. **Replication**: Forwards writes to secondary replicas
5. **Idempotency Tracking**: Maintains append state for exactly-once semantics
6. **Heartbeat Reporting**: Sends chunk inventory to master

### 3. Client Library

**Location**: [`internal/client/client.go`](internal/client/client.go)

Provides high-level APIs for file operations.

#### Core Functions

```go
// Push data to chunk server buffers
func PushDataTo(csAddr, dataID string, data []byte) (string, error)
func PushToReplicas(replicas []string, dataID string, data []byte) (string, error)

// Request primary to commit buffered data
func AppendRecord(primaryAddr, chunkHandle, dataID, appendID string, 
                  secondaries []string, version int64) (*pb.AppendRecordResponse, error)

// Read file content from chunks
func ReadFileChunks(localFile string, chunkSize int64) ([][]byte, error)
```

---

## Operations and Workflows

### 1. File Creation

**Workflow**:

```
Client â†’ Master: CreateFile(filename)
Master â†’ Check if file exists
Master â†’ Add to FileMap with empty chunk list
Master â†’ Log operation to WAL
Master â†’ Return success
```

**Implementation**: [`internal/master/master.go:368`](internal/master/master.go#L368)

```go
func (m *MasterServer) CreateFile(ctx context.Context, req *pb.CreateFileRequest) (*pb.Status, error) {
    m.meta.mu.Lock()
    if _, exists := m.meta.FileMap[req.Filename]; exists {
        m.meta.mu.Unlock()
        return &pb.Status{Code: 1, Message: "file already exists"}, errors.New("file exists")
    }
    m.meta.FileMap[req.Filename] = []string{} // Empty chunk list
    m.meta.mu.Unlock()
    
    if err := m.log.Append(opRecord{Op: "create", File: req.Filename, ChunkList: []string{}}); err != nil {
        return &pb.Status{Code: 1, Message: err.Error()}, err
    }
    m.dumpMetadataAsync()
    return &pb.Status{Code: 0, Message: "ok"}, nil
}
```

### 2. Record Append (Exactly-Once Semantics)

This is the most complex and critical operation in the system.

**High-Level Flow**:

```
1. Client reads local file and splits into 64MB chunks
2. Client requests chunk metadata from Master
3. Client pushes data to all replicas (pipelined)
4. Client sends AppendRecord to Primary
5. Primary executes two-phase commit:
   a. PREPARE phase: Reserve space on all replicas
   b. COMMIT phase: Write data at reserved offset
6. Primary returns success/failure
7. On failure, client retries with same idempotency ID
```

**Detailed Append Workflow**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client â”‚                â”‚ Master â”‚                â”‚ Primary â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                         â”‚                          â”‚
    â”‚ GetFileChunksInfo       â”‚                          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                          â”‚
    â”‚                         â”‚                          â”‚
    â”‚ <chunk locations>       â”‚                          â”‚
    â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                          â”‚
    â”‚                         â”‚                          â”‚
    â”‚ PushData (dataID, payload)                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
    â”‚                         â”‚                          â”‚
    â”‚ PushData (same dataID, same payload)               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚             â”‚
    â”‚                         â”‚             â”‚ Secondary  â”‚
    â”‚                         â”‚             â”‚            â”‚
    â”‚ AppendRecord(chunkHandle, dataID, idempotencyID)   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
    â”‚                         â”‚                          â”‚
    â”‚                         â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚                         â”‚     â”‚ Check idempotencyâ”‚ â”‚
    â”‚                         â”‚     â”‚ If committed:    â”‚ â”‚
    â”‚                         â”‚     â”‚   return offset  â”‚ â”‚
    â”‚                         â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚                         â”‚                          â”‚
    â”‚                         â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚                         â”‚     â”‚ PREPARE Phase    â”‚ â”‚
    â”‚                         â”‚     â”‚ - Reserve space  â”‚ â”‚
    â”‚                         â”‚     â”‚ - Forward to 2nd â”‚ â”‚
    â”‚                         â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚                         â”‚                          â”‚
    â”‚                         â”‚ ForwardAppend(PREPARE)   â”‚
    â”‚                         â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                         â”‚                          â”‚
    â”‚                         â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚                         â”‚     â”‚ COMMIT Phase     â”‚ â”‚
    â”‚                         â”‚     â”‚ - Write data     â”‚ â”‚
    â”‚                         â”‚     â”‚ - Forward commit â”‚ â”‚
    â”‚                         â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚                         â”‚                          â”‚
    â”‚                         â”‚ ForwardAppend(COMMIT)    â”‚
    â”‚                         â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                         â”‚                          â”‚
    â”‚ Success(offset, version)                           â”‚
    â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                         â”‚                          â”‚
```

**Implementation**: [`internal/chunkserver/handler.go:57`](internal/chunkserver/handler.go#L57)

```go
func (cs *ChunkServer) AppendRecord(ctx context.Context, req *pb.AppendRecordRequest) (*pb.AppendRecordResponse, error) {
    // 1. Validate idempotency ID
    if req.GetIdempotencyId() == "" {
        return &pb.AppendRecordResponse{Success: false}, fmt.Errorf("missing idempotency id")
    }
    
    // 2. Ensure we hold the lease
    version, err := cs.ensureLease(req.ChunkHandle)
    if err != nil {
        return &pb.AppendRecordResponse{Success: false}, err
    }
    
    // 3. Get buffered data
    data, err := cs.getBuffer(req.DataId)
    if err != nil {
        return &pb.AppendRecordResponse{Success: false}, err
    }
    
    // 4. Acquire chunk lock for serialization
    chunkLock := cs.getChunkLock(req.ChunkHandle)
    chunkLock.Lock()
    defer chunkLock.Unlock()
    
    // 5. Check if already committed (idempotency)
    if state, ok := cs.lookupAppendState(req.ChunkHandle, req.IdempotencyId); ok {
        switch state.status {
        case appendStatusCommitted:
            return cs.handleCommittedAppend(ctx, req, state)
        case appendStatusPrepared, appendStatusPreparing:
            // Abort stale state
            cs.abortLocalAppendLocked(req.ChunkHandle, req.IdempotencyId, state)
            cs.deleteAppendState(req.ChunkHandle, req.IdempotencyId)
        }
    }
    
    // 6. Get current offset
    offset, err := cs.currentChunkSize(req.ChunkHandle)
    if err != nil {
        return &pb.AppendRecordResponse{Success: false}, err
    }
    
    // 7. Create append state
    state := &appendRecordState{
        offset:       offset,
        length:       int64(len(data)),
        originalSize: offset,
        status:       appendStatusPreparing,
    }
    cs.saveAppendState(req.ChunkHandle, req.IdempotencyId, state)
    
    // 8. Reserve space locally
    if err := cs.reserveLocalAppendLocked(req.ChunkHandle, state); err != nil {
        cs.deleteAppendState(req.ChunkHandle, req.IdempotencyId)
        return &pb.AppendRecordResponse{Success: false}, err
    }
    cs.updateAppendStateStatus(req.ChunkHandle, req.IdempotencyId, appendStatusPrepared, 0)
    
    // 9. PREPARE phase: Forward to secondaries
    reservedLength := int64(len(data))
    for _, loc := range req.Secondaries {
        if err := cs.forwardAppendPhase(ctx, loc.Address, req, pb.AppendPhase_APPEND_PHASE_PREPARE, 
                                       offset, reservedLength, version); err != nil {
            // Abort on any failure
            cs.forwardAppendPhase(ctx, loc.Address, req, pb.AppendPhase_APPEND_PHASE_ABORT, 
                                 offset, reservedLength, version)
            cs.abortLocalAppendLocked(req.ChunkHandle, req.IdempotencyId, state)
            cs.deleteAppendState(req.ChunkHandle, req.IdempotencyId)
            return &pb.AppendRecordResponse{Success: false}, err
        }
    }
    
    // 10. COMMIT phase: Write data locally
    if err := cs.commitLocalAppendLocked(req.ChunkHandle, req.IdempotencyId, state, data, version); err != nil {
        // Abort on commit failure
        for _, loc := range req.Secondaries {
            cs.forwardAppendPhase(ctx, loc.Address, req, pb.AppendPhase_APPEND_PHASE_ABORT, 
                                 offset, reservedLength, version)
        }
        cs.deleteAppendState(req.ChunkHandle, req.IdempotencyId)
        return &pb.AppendRecordResponse{Success: false}, err
    }
    
    // 11. COMMIT phase: Forward to secondaries
    for _, loc := range req.Secondaries {
        if err := cs.forwardAppendPhase(ctx, loc.Address, req, pb.AppendPhase_APPEND_PHASE_COMMIT, 
                                       offset, reservedLength, version); err != nil {
            // Secondary commit failure - mark as committed but log error
            log.Printf("chunkserver %s: secondary %s failed commit for append id=%s", 
                      cs.addr, loc.Address, req.IdempotencyId)
        }
    }
    
    // 12. Mark as committed
    cs.updateAppendStateStatus(req.ChunkHandle, req.IdempotencyId, appendStatusCommitted, version)
    cs.touchAppendState(req.ChunkHandle, req.IdempotencyId)
    
    return &pb.AppendRecordResponse{Success: true, Offset: offset, CommittedVersion: version}, nil
}
```

### 3. Chunk Allocation

**Workflow**:

```
Client â†’ Master: AllocateChunk(filename)
Master â†’ Generate new chunk handle
Master â†’ Select 3 replica locations (random from healthy servers)
Master â†’ Update ChunkMap with new chunk info
Master â†’ Append chunk to file's chunk list
Master â†’ Log operation to WAL
Master â†’ Return chunk metadata to client
```

**Implementation**: [`internal/master/master.go:482`](internal/master/master.go#L482)

```go
func (m *MasterServer) AllocateChunk(ctx context.Context, req *pb.AllocateChunkRequest) (*pb.AllocateChunkResponse, error) {
    handle := m.newChunkHandle()
    
    // Select replicas
    cutoff := time.Now().Add(-replicaLivenessTimeout)
    candidates := m.meta.liveReplicaAddresses(cutoff)
    
    if len(candidates) < m.replicationFactor {
        return nil, fmt.Errorf("not enough live replicas")
    }
    
    // Random selection
    rand.Shuffle(len(candidates), func(i, j int) { 
        candidates[i], candidates[j] = candidates[j], candidates[i] 
    })
    replicas := candidates[:m.replicationFactor]
    
    // Update metadata
    m.meta.mu.Lock()
    chunks := append(m.meta.FileMap[req.Filename], handle)
    m.meta.FileMap[req.Filename] = chunks
    m.meta.mu.Unlock()
    
    ci := m.meta.EnsureChunk(handle)
    
    // Log operation
    if err := m.log.Append(opRecord{Op: "add_chunk", File: req.Filename, Chunk: handle}); err != nil {
        return nil, err
    }
    
    m.dumpMetadataAsync()
    
    // Return chunk locations
    return &pb.AllocateChunkResponse{
        ChunkHandle: handle,
        Locations:   buildLocations(replicas),
        Version:     ci.Version,
    }, nil
}
```

### 4. Lease Management

Leases prevent split-brain scenarios by ensuring only one primary can mutate a chunk at a time.

**Lease Request Flow**:

```
Primary â†’ Master: RequestLease(chunkHandle)
Master â†’ Check current lease status
Master â†’ If expired or no lease:
           - Increment chunk version
           - Grant lease to requester
           - Set expiration (60 seconds)
Master â†’ Return lease info (primary, secondaries, version, expiration)
```

**Implementation**: [`internal/master/master.go:533`](internal/master/master.go#L533)

```go
func (m *MasterServer) RequestLease(ctx context.Context, req *pb.RequestLeaseRequest) (*pb.RequestLeaseResponse, error) {
    handle := req.ChunkHandle
    requester := req.Requester
    
    primary, secondaries, expiration, hasLease := m.meta.GetLease(handle)
    
    // Check if valid lease exists
    if hasLease && time.Now().Before(expiration) {
        if primary != requester {
            return &pb.RequestLeaseResponse{Granted: false}, nil
        }
        // Extend existing lease
        newExp := time.Now().Add(m.leaseDuration)
        m.meta.SetLease(handle, primary, secondaries, newExp)
        ci := m.meta.EnsureChunk(handle)
        return &pb.RequestLeaseResponse{
            Granted:         true,
            Primary:         primary,
            Secondaries:     buildLocations(secondaries),
            Version:         ci.Version,
            LeaseExpiration: newExp.Unix(),
        }, nil
    }
    
    // Grant new lease
    cutoff := time.Now().Add(-replicaLivenessTimeout)
    locations := m.meta.HealthyChunkLocations(handle, cutoff)
    
    if len(locations) < 2 {
        return &pb.RequestLeaseResponse{Granted: false}, nil
    }
    
    // Pick secondaries (exclude requester)
    var secs []string
    for _, loc := range locations {
        if loc != requester {
            secs = append(secs, loc)
        }
    }
    if len(secs) > 2 {
        secs = secs[:2]
    }
    
    // Increment version and grant lease
    ci := m.meta.EnsureChunk(handle)
    ci.Version++
    newExp := time.Now().Add(m.leaseDuration)
    m.meta.SetLease(handle, requester, secs, newExp)
    
    return &pb.RequestLeaseResponse{
        Granted:         true,
        Primary:         requester,
        Secondaries:     buildLocations(secs),
        Version:         ci.Version,
        LeaseExpiration: newExp.Unix(),
    }, nil
}
```

### 5. Heartbeat Protocol

ChunkServers send periodic heartbeats to report health and chunk inventory.

**Heartbeat Flow**:

```
Every 5 seconds:
ChunkServer â†’ Master: HeartBeat(address, chunk_versions)
Master â†’ Update last_heartbeat timestamp
Master â†’ Update chunk locations
Master â†’ Detect stale replicas (version mismatch)
Master â†’ Return pending commands (e.g., delete chunk)
ChunkServer â†’ Execute commands
```

**Implementation**: [`internal/master/master.go:563`](internal/master/master.go#L563)

```go
func (m *MasterServer) HeartBeat(ctx context.Context, req *pb.HeartBeatRequest) (*pb.HeartBeatResponse, error) {
    addr := req.Address
    chunks := req.Chunks
    
    // Update replica state
    stale := m.meta.UpdateReplica(addr, chunks)
    
    // Check for version mismatches
    for _, handle := range stale {
        m.meta.MarkReplicaStale(handle, addr)
    }
    
    // Dequeue pending commands
    commands := m.dequeueCommands(addr)
    
    return &pb.HeartBeatResponse{Commands: commands}, nil
}
```

**ChunkServer Side**: [`internal/chunkserver/chunkserver.go:197`](internal/chunkserver/chunkserver.go#L197)

```go
func (cs *ChunkServer) sendHeartbeat() error {
    cs.mu.Lock()
    chunks := make(map[string]int64)
    for handle, version := range cs.chunkVersion {
        chunks[handle] = version
    }
    cs.mu.Unlock()
    
    conn, err := grpc.Dial(cs.masterAddr, grpc.WithTransportCredentials(insecure.NewCredentials()))
    if err != nil {
        return err
    }
    defer conn.Close()
    
    mc := pb.NewMasterClient(conn)
    resp, err := mc.HeartBeat(context.Background(), &pb.HeartBeatRequest{
        Address: cs.addr,
        Chunks:  chunks,
    })
    if err != nil {
        return err
    }
    
    // Execute commands from master
    for _, cmd := range resp.Commands {
        switch cmd.Command {
        case "delete":
            cs.deleteChunk(cmd.ChunkHandle)
        }
    }
    
    return nil
}
```

---

## Metadata Management

### File-to-Chunk Mapping

**Data Structure**: [`internal/master/metadata.go:31`](internal/master/metadata.go#L31)

```go
type MasterMetadata struct {
    mu             sync.RWMutex
    FileMap        map[string][]string           // filename â†’ ordered chunk handles
    ChunkMap       map[string]*ChunkInfo         // handle â†’ chunk metadata
    ChunkLocations map[string]map[string]struct{} // handle â†’ replica addresses
    Replicas       map[string]*ReplicaState      // address â†’ server state
    StaleReplicas  map[string]map[string]struct{} // handle â†’ stale replicas
}
```

**Example**:

```json
{
  "FileMap": {
    "/large-file.txt": [
      "ch-1764543124951407400-1",
      "ch-1764543124951407400-2",
      "ch-1764543124951407400-3"
    ]
  },
  "ChunkMap": {
    "ch-1764543124951407400-1": {
      "Handle": "ch-1764543124951407400-1",
      "Version": 5,
      "Primary": ":6001",
      "Secondaries": [":6002", ":6003"],
      "LeaseExpiration": "2025-12-01T04:59:00Z"
    }
  },
  "ChunkLocations": {
    "ch-1764543124951407400-1": {
      ":6001": {},
      ":6002": {},
      ":6003": {}
    }
  }
}
```

### Chunk Information

**Data Structure**: [`internal/master/metadata.go:8`](internal/master/metadata.go#L8)

```go
type ChunkInfo struct {
    Handle          string
    Version         int64         // Incremented on each lease grant
    Primary         string        // Current lease holder
    Secondaries     []string      // Replica addresses
    LeaseExpiration time.Time     // When lease expires
}
```

### Replica State Tracking

```go
type ReplicaState struct {
    Address       string
    LastHeartbeat time.Time
    Chunks        map[string]int64 // chunk handle â†’ version
}
```

### Metadata Persistence

The master dumps metadata snapshots every 10 seconds to [`metadata_snapshot.json`](metadata_snapshot.json).

**Snapshot Structure**:

```go
type metadataSnapshot struct {
    Timestamp      time.Time                   `json:"timestamp"`
    Files          map[string][]string         `json:"files"`
    Chunks         map[string]chunkSnapshot    `json:"chunks"`
    ChunkLocations map[string][]string         `json:"chunk_locations"`
    Replicas       map[string]replicaSnapshot  `json:"replicas"`
    StaleReplicas  map[string][]string         `json:"stale_replicas"`
    Garbage        []string                    `json:"garbage"`
}
```

**Implementation**: [`internal/master/master.go:227`](internal/master/master.go#L227)

---

## Exactly-Once Append Semantics

### Problem Statement

Traditional GFS provides **at-least-once** semantics, meaning:
- On network failures, clients retry appends
- Retries may succeed multiple times
- Result: Duplicate or missing data

This implementation provides **exactly-once** semantics using:

1. **Idempotency IDs**: Unique identifier per append operation
2. **Append State Tracking**: Primary maintains history of recent appends
3. **Two-Phase Commit**: Prepare â†’ Commit protocol
4. **Chunk-Level Locking**: Prevents concurrent append conflicts

### Idempotency ID Generation

**Client Side**: Uses UUID-like random strings

```go
func generateIdempotencyID() string {
    return fmt.Sprintf("append-%d-%d", time.Now().UnixNano(), rand.Int63())
}
```

### Append State Tracking

**Data Structure**: [`internal/chunkserver/append_state.go:17`](internal/chunkserver/append_state.go#L17)

```go
type appendRecordState struct {
    offset           int64        // Reserved offset
    length           int64        // Data length
    originalSize     int64        // Chunk size before reserve
    status           appendStatus // preparing, prepared, committed
    committedVersion int64        // Version when committed
    lastUpdated      time.Time    // For pruning
}

const (
    appendStatusPreparing appendStatus = iota
    appendStatusPrepared
    appendStatusCommitted
)
```

**State Lifecycle**:

```
NEW
 â”‚
 â”œâ”€> appendStatusPreparing (space reserved)
 â”‚
 â”œâ”€> appendStatusPrepared (secondaries prepared)
 â”‚
 â”œâ”€> appendStatusCommitted (data written)
 â”‚
 â””â”€> PRUNED (after 128 newer appends)
```

### Two-Phase Commit Protocol

**Phase 1: PREPARE**

```go
func (cs *ChunkServer) handleForwardPrepare(req *pb.ForwardAppendRequest) error {
    // Check if already committed
    if state, ok := cs.lookupAppendState(req.ChunkHandle, req.IdempotencyId); ok {
        if state.status == appendStatusCommitted {
            return nil // Already done
        }
        // Abort conflicting state
        cs.abortLocalAppendLocked(req.ChunkHandle, req.IdempotencyId, state)
        cs.deleteAppendState(req.ChunkHandle, req.IdempotencyId)
    }
    
    // Reserve space
    offset := req.Offset
    length := req.ReservedLength
    state := &appendRecordState{
        offset:       offset,
        length:       length,
        originalSize: currentSize,
        status:       appendStatusPrepared,
    }
    cs.saveAppendState(req.ChunkHandle, req.IdempotencyId, state)
    
    return cs.reserveLocalAppendLocked(req.ChunkHandle, state)
}
```

**Phase 2: COMMIT**

```go
func (cs *ChunkServer) handleForwardCommit(req *pb.ForwardAppendRequest) error {
    state, ok := cs.lookupAppendState(req.ChunkHandle, req.IdempotencyId)
    if !ok {
        return fmt.Errorf("no prepared state found")
    }
    
    if state.status == appendStatusCommitted {
        return nil // Idempotent
    }
    
    data, err := cs.getBuffer(req.DataId)
    if err != nil {
        return err
    }
    
    version := req.ChunkVersion
    if err := cs.commitLocalAppendLocked(req.ChunkHandle, req.IdempotencyId, state, data, version); err != nil {
        return err
    }
    
    cs.updateAppendStateStatus(req.ChunkHandle, req.IdempotencyId, appendStatusCommitted, version)
    return nil
}
```

### State Pruning

To prevent unbounded memory growth, the system keeps only the last 128 append states per chunk.

**Implementation**: [`internal/chunkserver/append_state.go:96`](internal/chunkserver/append_state.go#L96)

```go
func pruneAppendStates(bucket map[string]*appendRecordState) {
    if len(bucket) <= appendHistoryLimit {
        return
    }
    
    // Find oldest committed state
    var oldestKey string
    var oldestTime time.Time
    for key, state := range bucket {
        if state.status != appendStatusCommitted {
            continue
        }
        if oldestKey == "" || state.lastUpdated.Before(oldestTime) {
            oldestKey = key
            oldestTime = state.lastUpdated
        }
    }
    
    if oldestKey != "" {
        delete(bucket, oldestKey)
    }
}
```

### Persistent State

ChunkServers persist append states to disk for crash recovery.

**State File**: `<dataDir>/chunkserver_state.json`

**Structure**:

```json
{
  "chunk_sizes": {
    "ch-1764543124951407400-1": 67108864
  },
  "chunk_versions": {
    "ch-1764543124951407400-1": 5
  },
  "append_states": {
    "ch-1764543124951407400-1": {
      "append-1764543200000000000-12345": {
        "offset": 1024,
        "length": 512,
        "original_size": 1024,
        "status": 2,
        "committed_version": 5,
        "last_updated": "2025-12-01T04:30:00Z"
      }
    }
  }
}
```

**Sync Frequency**: Every 5 seconds or on state change

**Implementation**: [`internal/chunkserver/state_store.go:84`](internal/chunkserver/state_store.go#L84)

---

## Replication and Consistency

### Replication Factor

Default: **3-way replication**

Each chunk is stored on 3 different chunk servers for fault tolerance.

### Primary-Secondary Model

- **Primary**: Holds the lease, coordinates writes
- **Secondaries**: Replicate primary's operations

### Write Replication Flow

```
1. Client pushes data to Primary
2. Client pushes same data to Secondary-1
3. Client pushes same data to Secondary-2
4. Client sends AppendRecord to Primary
5. Primary forwards PREPARE to both secondaries
6. Primary waits for ACK from both
7. Primary commits locally
8. Primary forwards COMMIT to both secondaries
9. Primary returns success to client
```

### Consistency Guarantees

1. **Atomic Appends**: Either all replicas get the data or none
2. **Version Consistency**: All replicas maintain same version
3. **Ordering**: Primary serializes all operations with chunk locks
4. **Exactly-Once**: Idempotency tracking prevents duplicates

### Stale Replica Detection

When a chunk server misses updates (e.g., due to network partition), it becomes stale.

**Detection Mechanism**:

```go
func (m *MasterMetadata) UpdateReplica(address string, chunks map[string]int64) []string {
    var stale []string
    
    for handle, reportedVersion := range chunks {
        ci, ok := m.ChunkMap[handle]
        if !ok {
            continue
        }
        
        // Check version mismatch
        if reportedVersion < ci.Version {
            stale = append(stale, handle)
            m.markReplicaStaleLocked(handle, address)
        } else if reportedVersion > ci.Version {
            // Replica is ahead (shouldn't happen)
            ci.Version = reportedVersion
        }
    }
    
    return stale
}
```

### Lease Expiration and Failover

If a primary crashes or becomes unreachable:

1. Master detects missed heartbeats (10-second timeout)
2. Lease expires (60-second TTL)
3. New primary requests lease
4. Master grants lease with incremented version
5. New primary becomes authoritative

---

## Failure Handling and Recovery

### 1. ChunkServer Crash

**Scenario**: ChunkServer goes offline mid-operation

**Handling**:

1. **Heartbeat Timeout**: Master detects failure within 10 seconds
2. **Lease Revocation**: If crashed server was primary, lease expires
3. **Re-replication**: Master should trigger re-replication (not fully implemented)
4. **Client Retry**: Client retries append, gets new lease holder

### 2. Master Crash

**Scenario**: Master server crashes and loses in-memory state

**Handling**:

1. **WAL Replay**: On restart, master replays [`operation.log`](operation.log)
2. **Metadata Recovery**: Rebuilds file-to-chunk mappings
3. **Heartbeat Re-sync**: ChunkServers report inventory on next heartbeat
4. **Lease Recovery**: All leases expire (60s max), new leases granted on demand

**WAL Replay Implementation**: [`internal/master/operation_log.go:65`](internal/master/operation_log.go#L65)

```go
func (ol *OperationLog) Replay(path string, meta *MasterMetadata) error {
    f, err := os.Open(path)
    if err != nil {
        return nil // No log to replay
    }
    defer f.Close()
    
    sc := bufio.NewScanner(f)
    for sc.Scan() {
        var r opRecord
        if err := json.Unmarshal(sc.Bytes(), &r); err != nil {
            continue
        }
        
        switch r.Op {
        case "create":
            meta.SetFileChunks(r.File, r.ChunkList)
        case "delete":
            delete(meta.FileMap, r.File)
        case "rename":
            if chunks, ok := meta.FileMap[r.File]; ok {
                meta.FileMap[r.NewName] = chunks
                delete(meta.FileMap, r.File)
            }
        case "add_chunk":
            chunks := meta.FileMap[r.File]
            chunks = append(chunks, r.Chunk)
            meta.FileMap[r.File] = chunks
            meta.EnsureChunk(r.Chunk)
        }
    }
    return nil
}
```

### 3. Network Partition

**Scenario**: Primary and secondaries can't communicate

**Handling**:

1. **Prepare Phase Failure**: Primary aborts append, returns failure
2. **Client Retry**: Client retries with exponential backoff
3. **Lease Expiration**: If partition persists, lease expires
4. **New Primary**: Master grants lease to accessible replica

### 4. Lost Acknowledgement

**Scenario**: Append succeeds but ACK is lost in network

**Handling**:

1. **Client Timeout**: Client thinks append failed
2. **Client Retry**: Sends same append with same idempotency ID
3. **Primary Lookup**: Finds committed state for that ID
4. **Return Cached Result**: Returns original offset without re-executing

**Test Validation**: [`internal/chunkserver/append_test.go`](internal/chunkserver/append_test.go) - Test 1

### 5. Partial Replica Failure

**Scenario**: One secondary fails during prepare phase

**Handling**:

1. **Primary Detects Failure**: gRPC timeout on ForwardAppend
2. **Abort Protocol**: Primary sends ABORT to all secondaries
3. **Rollback**: Secondaries truncate reserved space
4. **Return Failure**: Primary returns failure to client
5. **Client Retry**: Retries, possibly with different secondaries

**Test Validation**: [`internal/chunkserver/append_test.go`](internal/chunkserver/append_test.go) - Test 2

### 6. Secondary Failure After PREPARE Before COMMIT

**Scenario**: Secondary successfully responds to PREPARE message but crashes before receiving COMMIT message

This is a critical edge case in the two-phase commit protocol that tests the system's resilience.

**Timeline**:

```
T0: Client sends AppendRecord to Primary
T1: Primary reserves space locally (PREPARE)
T2: Primary sends PREPARE to Secondary-1 âœ…
T3: Secondary-1 reserves space and responds OK âœ…
T4: Primary sends PREPARE to Secondary-2 âœ…
T5: Secondary-2 reserves space and responds OK âœ…
T6: Primary commits data locally
T7: Primary sends COMMIT to Secondary-1
T8: Secondary-1 crashes ğŸ’¥ (before receiving COMMIT)
T9: Primary sends COMMIT to Secondary-2 âœ…
T10: Primary returns success to client
```

**Handling**:

1. **Primary Perspective**:
   - Primary successfully prepared all replicas
   - Primary commits locally and to reachable secondaries
   - COMMIT to Secondary-1 fails (connection refused/timeout)
   - **Decision**: Primary logs error but still returns success to client
   - Rationale: Data is committed on primary and at least one secondary (2/3 replicas)

2. **Failed Secondary State**:
   - Has reserved space (state = `appendStatusPrepared`)
   - Chunk size extended by reserved length
   - No actual data written at the offset
   - **Result**: "Hole" in the chunk at that offset

3. **Recovery Process**:
   - When Secondary-1 restarts, it loads state from `chunkserver_state.json`
   - Finds append state with status = `prepared` (not committed)
   - **Option A**: Abort on restart and truncate reserved space
   - **Option B**: Keep prepared state and wait for retry/resolution
   - **Current Implementation**: State persists until pruned or overwritten

4. **Consistency Implications**:
   - **Primary**: Has correct data at offset X
   - **Secondary-1**: Has hole/garbage at offset X
   - **Secondary-2**: Has correct data at offset X
   - **Version Inconsistency**: All replicas have same version (incremented during lease)
   - **Data Inconsistency**: Secondary-1 has different bytes than primary

5. **Detection and Repair**:
   - **Heartbeat Detection**: Master detects Secondary-1 missed heartbeats
   - **Stale Marking**: After restart, Secondary-1 reports old version â†’ marked stale
   - **Read Impact**: Reads from Secondary-1 may return corrupted data
   - **Solution**: Re-replication from primary or healthy secondary

**Code Path**: [`internal/chunkserver/handler.go:176-190`](internal/chunkserver/handler.go)

```go
// COMMIT phase: Forward to secondaries
for _, loc := range req.Secondaries {
    if err := cs.forwardAppendPhase(ctx, loc.Address, req, pb.AppendPhase_APPEND_PHASE_COMMIT, 
                                   offset, reservedLength, version); err != nil {
        // Secondary commit failure - mark as committed but log error
        log.Printf("chunkserver %s: secondary %s failed commit for append id=%s", 
                  cs.addr, loc.Address, req.IdempotencyId)
        // CONTINUES despite error - does NOT abort
    }
}
```

**Mitigation Strategies**:

1. **Checksums**: Add chunk checksums to detect corruption during reads
2. **Scrubbing**: Periodic background verification of replica consistency
3. **Read Repair**: On read mismatch, copy data from healthy replica
4. **Aggressive Re-replication**: Immediately re-replicate on detected inconsistency

**Test Scenario**:

```go
// Simulate: Secondary ACKs PREPARE, then crashes before COMMIT
func TestSecondaryFailureAfterPrepare(t *testing.T) {
    // Setup: Primary + 2 Secondaries
    // 1. Client pushes data to all replicas
    // 2. Client sends AppendRecord to Primary
    // 3. Primary sends PREPARE to both secondaries
    // 4. Both secondaries reserve space and ACK
    // 5. Crash Secondary-1 (stop process)
    // 6. Primary commits locally
    // 7. Primary sends COMMIT (fails to Secondary-1, succeeds to Secondary-2)
    // 8. Verify: Primary returns success
    // 9. Verify: Secondary-1 has prepared state (after restart)
    // 10. Verify: Secondary-2 has committed data
}
```

### 7. Primary Failure After Receiving COMMIT Command

**Scenario**: Primary receives COMMIT message from all secondaries but crashes before sending ACK to client

This scenario tests the idempotency guarantee when the operation succeeds but the client doesn't know.

**Timeline**:

```
T0: Client sends AppendRecord(idempotencyID="append-12345")
T1: Primary receives request
T2: Primary executes PREPARE phase successfully
T3: All secondaries ACK PREPARE
T4: Primary commits data locally âœ…
T5: Primary sends COMMIT to all secondaries
T6: All secondaries commit successfully âœ…
T7: All secondaries send ACK to Primary âœ…
T8: Primary receives all ACKs âœ…
T9: Primary crashes ğŸ’¥ (before sending response to client)
T10: Client timeout expires
T11: Client retries with same idempotencyID
```

**Handling**:

1. **Client Perspective**:
   - Never receives success/failure response
   - After timeout (e.g., 5 seconds), assumes failure
   - **Must retry** with same idempotency ID to preserve exactly-once semantics
   - Cannot know if operation succeeded or failed

2. **System State After Crash**:
   - **Primary**: Crashed, data committed to disk
   - **Secondary-1**: Data committed, state = `appendStatusCommitted`
   - **Secondary-2**: Data committed, state = `appendStatusCommitted`
   - **All replicas**: Have correct data at offset X
   - **Idempotency tracking**: Primary's in-memory state lost (if not persisted)

3. **Recovery Process**:

   **When Primary Restarts**:
   ```
   Step 1: Load state from chunkserver_state.json
   Step 2: Recover append_states map
   Step 3: Find entry for "append-12345" with status=committed
   Step 4: Chunk version and data already on disk
   ```

   **When Client Retries**:
   ```
   Client â†’ Primary: AppendRecord(idempotencyID="append-12345")
   Primary â†’ Check append_states["chunk-handle"]["append-12345"]
   Primary â†’ Found with status=committed
   Primary â†’ Return cached result (offset, version)
   Primary â†’ NO duplicate write
   ```

4. **Persistent State Requirement**:

   For perfect exactly-once across crashes, append states MUST be persisted.

   **Current Implementation**: [`internal/chunkserver/state_store.go:84`](internal/chunkserver/state_store.go)
   
   ```go
   func (cs *ChunkServer) syncStateToDisk() error {
       if !cs.stateDirty.Load() {
           return nil
       }
       
       snapshot := cs.buildStateSnapshot()
       data, err := json.MarshalIndent(snapshot, "", "  ")
       if err != nil {
           return err
       }
       
       // Atomic write: temp file + rename
       tempPath := cs.statePath + ".tmp"
       if err := os.WriteFile(tempPath, data, 0644); err != nil {
           return err
       }
       if err := os.Rename(tempPath, cs.statePath); err != nil {
           return err
       }
       
       cs.stateDirty.Store(false)
       return nil
   }
   ```

   **Sync Frequency**: Background goroutine syncs every 5 seconds

5. **Lease Expiration Complication**:

   If primary crash lasts > 60 seconds:
   - Lease expires
   - New primary elected
   - New primary doesn't have old idempotency state
   - **Risk**: Retry could duplicate operation

   **Mitigation**:
   - Secondary promotion inherits append states
   - Or: Persist states to shared storage (e.g., etcd)

6. **Three Possible Client Retry Outcomes**:

   **Case A: Primary restarts quickly (< 60s)**
   ```
   Client retry â†’ Old Primary (still holds lease)
   Primary checks append_states â†’ Found committed
   Returns: Success(offset, version) [cached]
   Result: âœ… Exactly-once preserved
   ```

   **Case B: Primary dead, new primary elected**
   ```
   Client retry â†’ New Primary
   New Primary checks append_states â†’ Not found
   New Primary executes append â†’ Duplicate write
   Result: âŒ At-least-once (duplicate)
   ```

   **Case C: Primary restarts after lease expires**
   ```
   Client retry â†’ Old Primary (no lease)
   Primary returns: Lease error
   Client requests metadata from Master
   Client â†’ New Primary
   â†’ Same as Case B
   ```

**Code Path for Idempotency Check**: [`internal/chunkserver/handler.go:77-82`](internal/chunkserver/handler.go)

```go
// Check if already committed (idempotency)
if state, ok := cs.lookupAppendState(req.ChunkHandle, req.IdempotencyId); ok {
    switch state.status {
    case appendStatusCommitted:
        return cs.handleCommittedAppend(ctx, req, state)
    // ...
}
```

**handleCommittedAppend Implementation**:

```go
func (cs *ChunkServer) handleCommittedAppend(ctx context.Context, 
    req *pb.AppendRecordRequest, state *appendRecordState) (*pb.AppendRecordResponse, error) {
    
    log.Printf("chunkserver %s: append %s already committed at offset %d", 
              cs.addr, req.IdempotencyId, state.offset)
    
    // Return cached result without re-executing
    return &pb.AppendRecordResponse{
        Success:          true,
        Offset:           state.offset,
        CommittedVersion: state.committedVersion,
    }, nil
}
```

**Test Validation**:

This scenario is covered by Test #1 in [`internal/chunkserver/append_test.go`](internal/chunkserver/append_test.go):

```go
func TestLostAcknowledgement(t *testing.T) {
    // 1. First append succeeds fully
    resp1, err := cs.AppendRecord(ctx, &req)
    require.NoError(t, err)
    require.True(t, resp1.Success)
    
    // 2. Simulate lost ACK: client doesn't see response
    // Client retries with SAME idempotency ID
    resp2, err := cs.AppendRecord(ctx, &req)
    require.NoError(t, err)
    require.True(t, resp2.Success)
    
    // 3. Verify idempotency: same offset returned
    assert.Equal(t, resp1.Offset, resp2.Offset)
    assert.Equal(t, resp1.CommittedVersion, resp2.CommittedVersion)
    
    // 4. Verify no duplicate write
    size := getChunkSize(chunkHandle)
    expectedSize := initialSize + dataLength
    assert.Equal(t, expectedSize, size)
}
```

**Guarantees Provided**:

âœ… **Within Lease Period**: Perfect exactly-once (idempotency cache hit)  
âš ï¸ **After Lease Expiration**: Falls back to at-least-once (potential duplicate)  
âœ… **No Data Loss**: All committed appends are durable  
âœ… **No Silent Failure**: Client always gets definitive answer on retry  

### 8. Under-Replication Scenario

**Scenario**: Chunk has fewer than the desired replication factor (3) due to server failures

**Causes**:

1. **Chunk Server Permanent Failure**: Server dies and doesn't come back
2. **Multiple Concurrent Failures**: 2+ servers fail simultaneously
3. **Disk Failure**: Server is alive but lost chunk data
4. **Network Partition**: Server unreachable but not failed

**Detection**:

The master detects under-replication through heartbeat monitoring:

```go
func (m *MasterServer) detectUnderReplication() map[string]int {
    underReplicated := make(map[string]int)
    
    m.meta.mu.RLock()
    defer m.meta.mu.RUnlock()
    
    for handle, locations := range m.meta.ChunkLocations {
        activeCount := 0
        cutoff := time.Now().Add(-replicaLivenessTimeout)
        
        for addr := range locations {
            if replica, ok := m.meta.Replicas[addr]; ok {
                if replica.LastHeartbeat.After(cutoff) {
                    activeCount++
                }
            }
        }
        
        if activeCount < m.replicationFactor {
            underReplicated[handle] = m.replicationFactor - activeCount
        }
    }
    
    return underReplicated
}
```

**Example Timeline**:

```
T0: Chunk "ch-12345" has 3 replicas: :6000, :6001, :6002
T1: Server :6001 crashes
T2: Master detects missing heartbeat from :6001 (after 10 seconds)
T3: Master marks :6001 as dead
T4: Chunk "ch-12345" now has 2 active replicas (under-replicated)
T5: Master should trigger re-replication
```

**Current System Behavior**:

âš ï¸ **Limitation**: The current implementation does NOT automatically re-replicate under-replicated chunks.

**Impact**:

1. **Reduced Fault Tolerance**: Only 2 copies exist instead of 3
2. **Risk of Data Loss**: If one more replica fails, data is lost
3. **Read Availability**: Reads still work from remaining replicas
4. **Write Availability**: Writes work with 2 secondaries (or 1 primary + 1 secondary)

**Manual Re-replication** (Conceptual):

```go
func (m *MasterServer) reReplicateChunk(handle string, count int) error {
    // 1. Find healthy source replica
    source := m.meta.FindHealthyReplica(handle)
    if source == "" {
        return fmt.Errorf("no healthy replica found")
    }
    
    // 2. Select new target servers
    targets := m.meta.SelectReplicationTargets(handle, count)
    if len(targets) < count {
        return fmt.Errorf("not enough available servers")
    }
    
    // 3. Issue copy commands
    for _, target := range targets {
        cmd := &pb.HeartBeatCommand{
            Command:     "copy_chunk",
            ChunkHandle: handle,
            SourceAddr:  source,
        }
        m.enqueueCommand(target, cmd)
    }
    
    return nil
}
```

**Future Enhancement**:

Automatic re-replication could be implemented as a background goroutine:

```go
func (m *MasterServer) replicationLoop() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        underReplicated := m.detectUnderReplication()
        
        for handle, needed := range underReplicated {
            log.Printf("master: chunk %s under-replicated (need %d more replicas)", 
                      handle, needed)
            
            if err := m.reReplicateChunk(handle, needed); err != nil {
                log.Printf("master: failed to re-replicate %s: %v", handle, err)
            }
        }
    }
}
```

### 9. Over-Replication Scenario

**Scenario**: Chunk has more than the desired replication factor (3) due to transient failures and recovery

**Causes**:

1. **Network Partition Healing**: Server was thought dead but comes back
2. **Premature Re-replication**: System re-replicated before server recovered
3. **Manual Intervention**: Admin manually copied chunk
4. **Race Condition**: Re-replication triggered while server was restarting

**Example Timeline**:

```
T0: Chunk "ch-12345" has 3 replicas: :6000, :6001, :6002
T1: Server :6001 experiences network partition (can't reach master)
T2: Master detects missing heartbeat (after 10 seconds)
T3: Master marks :6001 as dead
T4: Master triggers re-replication to :6003
T5: Chunk now on: :6000, :6002, :6003 (3 replicas)
T6: Network partition heals - :6001 reconnects
T7: :6001 sends heartbeat with chunk inventory including "ch-12345"
T8: Master receives heartbeat, updates locations
T9: Chunk now on: :6000, :6001, :6002, :6003 (4 replicas - over-replicated!)
```

**Detection**:

```go
func (m *MasterServer) detectOverReplication() map[string][]string {
    overReplicated := make(map[string][]string)
    
    m.meta.mu.RLock()
    defer m.meta.mu.RUnlock()
    
    for handle, locations := range m.meta.ChunkLocations {
        activeCount := 0
        var activeAddrs []string
        cutoff := time.Now().Add(-replicaLivenessTimeout)
        
        for addr := range locations {
            if replica, ok := m.meta.Replicas[addr]; ok {
                if replica.LastHeartbeat.After(cutoff) {
                    activeCount++
                    activeAddrs = append(activeAddrs, addr)
                }
            }
        }
        
        if activeCount > m.replicationFactor {
            // Store excess replicas for deletion
            excess := activeCount - m.replicationFactor
            overReplicated[handle] = activeAddrs[m.replicationFactor:]
        }
    }
    
    return overReplicated
}
```

**Current System Behavior**:

The master accepts extra replicas without issue:

1. **ChunkLocations Map**: Grows to include all reporting replicas
2. **No Automatic Deletion**: Extra replicas are not automatically removed
3. **Lease Selection**: Only 3 replicas used for writes (primary + 2 secondaries)
4. **Read Load Balancing**: Extra replicas can serve reads (benefit!)

**Handling Strategy**:

**Option A: Keep Extra Replicas (Current)**
- **Pros**: Better read availability, higher fault tolerance
- **Cons**: Wasted storage, metadata overhead

**Option B: Delete Excess Replicas**
- **Pros**: Consistent replication factor, saves storage
- **Cons**: Requires careful selection (preserve data locality, load balance)

**Smart Deletion Logic** (Conceptual):

```go
func (m *MasterServer) deleteExcessReplicas(handle string, excess []string) {
    // Criteria for choosing which replicas to delete:
    // 1. Prefer deleting from least-loaded servers
    // 2. Preserve geographic diversity
    // 3. Don't delete current primary
    // 4. Don't delete recently active replicas
    
    ci := m.meta.ChunkMap[handle]
    
    var candidates []string
    for _, addr := range excess {
        // Don't delete primary
        if addr == ci.Primary {
            continue
        }
        
        // Don't delete if recently used in lease
        isSecondary := false
        for _, sec := range ci.Secondaries {
            if addr == sec {
                isSecondary = true
                break
            }
        }
        if isSecondary {
            continue
        }
        
        candidates = append(candidates, addr)
    }
    
    // Delete least-loaded replicas
    sort.Slice(candidates, func(i, j int) bool {
        loadI := m.meta.Replicas[candidates[i]].ChunkCount()
        loadJ := m.meta.Replicas[candidates[j]].ChunkCount()
        return loadI > loadJ // Higher load = delete first
    })
    
    for _, addr := range candidates {
        cmd := &pb.HeartBeatCommand{
            Command:     "delete",
            ChunkHandle: handle,
        }
        m.enqueueCommand(addr, cmd)
        
        // Update metadata
        delete(m.meta.ChunkLocations[handle], addr)
    }
}
```

**Impact Analysis**:

| Scenario | Storage Cost | Read Perf | Write Perf | Fault Tolerance |
|----------|--------------|-----------|------------|-----------------|
| Exact 3 replicas | 3x | Baseline | Baseline | 2 failures tolerated |
| 4 replicas | 4x (+33%) | +25% | Same | 3 failures tolerated |
| 5 replicas | 5x (+67%) | +67% | Same | 4 failures tolerated |

**Recommended Approach**:

1. **Soft Limit**: Allow temporary over-replication (up to +1 or +2)
2. **Lazy Deletion**: Delete excess only after grace period (e.g., 10 minutes)
3. **Smart Selection**: Use load-based deletion criteria
4. **Monitoring**: Alert operators if over-replication persists

**Implementation Priority**: Low (over-replication is generally harmless)

## Performance Optimizations

### Test Suite

**Location**: [`internal/chunkserver/append_test.go`](internal/chunkserver/append_test.go)

The project includes comprehensive test scenarios:

1. **Lost Acknowledgement** - Verifies idempotency on retry
2. **Partial Replica Failure** - Tests abort and rollback
3. **Primary Crash After Commit** - Validates state recovery
4. **Slow Primary (Timeout)** - Concurrent operation handling
5. **Concurrent Retries** - Race condition prevention
6. **Multiple Different Appends** - Unique ID isolation
7. **Idempotency ID Reuse** - Cache hit validation
8. **Stress Test** - 10 concurrent clients, 2 ops each

### Test Results

**Summary**: All 8 tests pass successfully

| Test | Status | Duration | Validation |
|------|--------|----------|------------|
| Lost ACK | âœ… PASS | 0.01s | Idempotent retry |
| Partial Failure | âœ… PASS | 0.01s | Proper abort |
| Crash Recovery | âœ… PASS | 0.01s | State persistence |
| Timeout | âœ… PASS | 2.01s | Concurrent ops |
| Concurrency | âœ… PASS | 0.00s | No race conditions |
| Multiple Appends | âœ… PASS | 0.01s | Unique writes |
| ID Reuse | âœ… PASS | 0.00s | Cache lookup |
| Stress Test | âœ… PASS | 0.02s | 20 operations |

**Total Test Time**: ~2.3 seconds  
**Success Rate**: 100%

---

## Benchmarking Results

### Benchmark Suite

**Location**: [`benchmarks/`](benchmarks/)

The project includes two benchmark types:

1. **Throughput Benchmark** ([`benchmarks/throughput/throughput_bench.go`](benchmarks/throughput/throughput_bench.go))
2. **Latency Benchmark** ([`benchmarks/latency/latency_bench.go`](benchmarks/latency/latency_bench.go))

### Throughput Results

**Test Configuration**:
- Duration: 20 seconds per test
- Payload: 1KB per operation
- Clients: 1, 2, 3, 4

**Results** (from [`temp_throughput.csv`](temp_throughput.csv)):

| Clients | Throughput (ops/s) | Throughput (MB/s) | Avg Latency (ms) | Max Latency (ms) |
|---------|-------------------|-------------------|------------------|------------------|
| 1 | 16.57 | 0.02 | 59.86 | 232.0 |
| 2 | 25.46 | 0.02 | 77.98 | 323.0 |
| 3 | 23.01 | 0.02 | 129.6 | 385.0 |
| 4 | 22.88 | 0.02 | 173.46 | 493.0 |

**Analysis**:
- Peak throughput at 2 clients (~25 ops/s)
- Latency increases with client count (coordination overhead)
- System is CPU/network bound, not disk bound

### Latency Results

**Test Configuration**:
- Operations: 100-200 per test
- Payload: 1KB
- Clients: 1, 2, 3

**Results** (from [`temp_latency.csv`](temp_latency.csv)):

| Clients | Avg Latency | P50 | P95 | P99 | Max |
|---------|-------------|-----|-----|-----|-----|
| 1 | 47.34ms | 84.26ms | 202.75ms | 314.71ms | 314.71ms |
| 2 | 46.66ms | 95.54ms | 218.73ms | 318.74ms | 320.62ms |
| 3 | 19.91ms | 97.32ms | 238.07ms | 320.47ms | 323.35ms |

**Analysis**:
- Median latency ~80-100ms (includes network + 2PC)
- P99 latency ~300-320ms (acceptable for distributed system)
- Tail latencies stable across client counts

### Performance Bottlenecks

1. **Two-Phase Commit Overhead**: ~50ms added latency
2. **Network Round Trips**: 3-4 round trips per append
3. **Disk Sync**: `fsync()` on commit adds ~10-20ms
4. **Lease Acquisition**: First operation on chunk pays lease cost

---

## Garbage Collection

### Orphan Chunk Detection

When a file is deleted, its chunks become orphans.

**Detection**: [`internal/master/master.go:172`](internal/master/master.go#L172)

```go
func (m *MasterServer) rebuildGarbageCandidates() {
    m.meta.mu.RLock()
    referenced := make(map[string]struct{})
    for _, chunks := range m.meta.FileMap {
        for _, handle := range chunks {
            referenced[handle] = struct{}{}
        }
    }
    
    var orphans []string
    for handle := range m.meta.ChunkMap {
        if _, ok := referenced[handle]; !ok {
            orphans = append(orphans, handle)
        }
    }
    m.meta.mu.RUnlock()
    
    for _, handle := range orphans {
        m.markChunkForGC(handle)
    }
}
```

### Grace Period

Chunks are marked for deletion but not immediately removed.

**Grace Period**: 120 seconds

This allows for:
- In-flight operations to complete
- Metadata inconsistencies to resolve
- Accidental deletions to be recovered

**GC Loop**: [`internal/master/master.go:99`](internal/master/master.go#L99)

```go
func (m *MasterServer) gcLoop() {
    ticker := time.NewTicker(m.gcInterval) // 30 seconds
    defer ticker.Stop()
    
    for range ticker.C {
        m.collectGarbage()
    }
}

func (m *MasterServer) collectGarbage() {
    now := time.Now()
    var ready []string
    
    m.gcMu.Lock()
    for handle, marked := range m.garbage {
        if now.Sub(marked) >= m.gcGracePeriod { // 120 seconds
            ready = append(ready, handle)
            delete(m.garbage, handle)
        }
    }
    m.gcMu.Unlock()
    
    // Send delete commands to chunk servers
    for _, handle := range ready {
        locations := m.meta.GetChunkLocations(handle)
        m.enqueueDeleteCommand(handle, locations)
        m.meta.RemoveChunk(handle)
        m.leases.Revoke(handle)
    }
}
```

### Delete Command Queue

Master enqueues delete commands and sends them via heartbeat responses.

```go
func (m *MasterServer) enqueueDeleteCommand(handle string, locations []string) {
    m.commandMu.Lock()
    defer m.commandMu.Unlock()
    
    for _, addr := range locations {
        cmd := &pb.HeartBeatCommand{
            Command:     "delete",
            ChunkHandle: handle,
        }
        m.commandQueue[addr] = append(m.commandQueue[addr], cmd)
    }
}
```

---

## Command Queue System

The master uses a command queue to send instructions to chunk servers asynchronously.

### Queue Operations

1. **Enqueue**: Master adds command to server's queue
2. **Dequeue**: ChunkServer retrieves commands during heartbeat
3. **Execute**: ChunkServer processes commands and reports back

### Supported Commands

Currently, only `delete` is implemented:

```protobuf
message HeartBeatCommand {
  string command = 1;       // "delete"
  string chunk_handle = 2;  // Chunk to delete
}
```

### Future Command Types

The architecture supports:
- `re_replicate`: Trigger re-replication
- `update_version`: Force version update
- `snapshot`: Create chunk snapshot
- `migrate`: Move chunk to different server

---

## Limitations and Future Work

### Current Limitations

1. **No Automatic Re-replication**: System doesn't automatically re-replicate under-replicated chunks (see [Under-Replication Scenario](#8-under-replication-scenario))
2. **No Read Operation in CLI**: Read API exists but CLI doesn't expose it
3. **In-Memory Append State**: Crash after lease expiration loses idempotency history (see [Primary Failure After Receiving COMMIT](#7-primary-failure-after-receiving-commit-command))
4. **Single Master**: Master is a single point of failure
5. **No Client Caching**: Clients don't cache metadata
6. **No Snapshot Support**: Can't create point-in-time snapshots
7. **No Replica Scrubbing**: Missing background verification for consistency (see [Secondary Failure After PREPARE](#6-secondary-failure-after-prepare-before-commit))

### Future Enhancements

1. **Automatic Re-replication**
   ```go
   func (m *MasterServer) replicateUnderreplicated() {
       for handle, locations := range m.meta.ChunkLocations {
           if len(locations) < m.replicationFactor {
               m.triggerReplication(handle, m.replicationFactor - len(locations))
           }
       }
   }
   ```

2. **Master Replication (Raft/Paxos)**
   - Use Raft for master consensus
   - Eliminate single point of failure

3. **Client-Side Caching**
   - Cache chunk locations locally
   - Reduce master load

4. **Persistent Idempotency State**
   - Store append states in distributed KV store (e.g., etcd)
   - Perfect exactly-once across crashes

5. **Read API in CLI**
   ```go
   func handleRead(args []string) {
       // Already implemented in client library
       chunks := getFileChunks(filename)
       for _, chunk := range chunks {
           data := readChunk(chunk.primary, chunk.handle)
           os.Stdout.Write(data)
       }
   }
   ```

6. **Chunk Checksums**
   - Detect corrupted data
   - Trigger re-replication from healthy replicas

7. **Compression**
   - Reduce storage and network overhead

---

## Conclusion

This project successfully implements a distributed file system with **exactly-once append semantics**, addressing one of the key limitations of the original GFS design. The system demonstrates:

### Technical Achievements

âœ… **Robust Distributed Coordination** via lease-based primary election  
âœ… **Strong Consistency Guarantees** through two-phase commit  
âœ… **Fault Tolerance** with 3-way replication and crash recovery  
âœ… **Scalable Architecture** supporting multiple concurrent clients  
âœ… **Production-Ready Features** including WAL, heartbeats, and GC  

### Performance Characteristics

- **Throughput**: 25 ops/sec (2 clients, 1KB payload)
- **Latency**: P50 = 95ms, P99 = 320ms
- **Consistency**: 100% exactly-once guarantee in all test scenarios
- **Availability**: Survives single chunk server failure

### Lessons Learned

1. **Trade-offs Matter**: In-memory idempotency state trades crash tolerance for performance
2. **2PC is Expensive**: Adds ~50ms latency but ensures correctness
3. **Testing is Critical**: Comprehensive test suite caught edge cases early
4. **Simplicity Wins**: Avoided complex 3PC in favor of simpler 2PC with idempotency

### Educational Value

This project provides hands-on experience with:
- Distributed consensus and coordination
- Replication protocols
- Failure handling in distributed systems
- gRPC and protocol buffers
- System design trade-offs

---

## Appendix

### A. File Structure

```
.
â”œâ”€â”€ api/proto/                  # Protocol buffer definitions
â”‚   â”œâ”€â”€ chunkserver.proto       # ChunkServer RPC service
â”‚   â”œâ”€â”€ master.proto            # Master RPC service
â”‚   â””â”€â”€ common.proto            # Shared messages
â”‚
â”œâ”€â”€ cmd/                        # Entry points
â”‚   â”œâ”€â”€ master/                 # Master server executable
â”‚   â”œâ”€â”€ chunkserver/            # ChunkServer executable
â”‚   â”œâ”€â”€ client/                 # CLI client
â”‚   â””â”€â”€ interactive/            # Interactive REPL
â”‚
â”œâ”€â”€ internal/                   # Internal packages
â”‚   â”œâ”€â”€ master/                 # Master implementation
â”‚   â”‚   â”œâ”€â”€ master.go           # Core logic
â”‚   â”‚   â”œâ”€â”€ metadata.go         # Metadata management
â”‚   â”‚   â””â”€â”€ operation_log.go    # WAL implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ chunkserver/            # ChunkServer implementation
â”‚   â”‚   â”œâ”€â”€ chunkserver.go      # Core logic
â”‚   â”‚   â”œâ”€â”€ handler.go          # RPC handlers
â”‚   â”‚   â”œâ”€â”€ append_state.go     # Idempotency tracking
â”‚   â”‚   â””â”€â”€ state_store.go      # Persistent state
â”‚   â”‚
â”‚   â”œâ”€â”€ client/                 # Client library
â”‚   â”‚   â””â”€â”€ client.go           # High-level APIs
â”‚   â”‚
â”‚   â””â”€â”€ config/                 # Configuration
â”‚       â””â”€â”€ config.go           # YAML loader
â”‚
â”œâ”€â”€ benchmarks/                 # Performance benchmarks
â”‚   â”œâ”€â”€ latency/                # Latency tests
â”‚   â”œâ”€â”€ throughput/             # Throughput tests
â”‚   â””â”€â”€ benchutil/              # Shared utilities
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml             # System configuration
â”‚
â”œâ”€â”€ Makefile                    # Build automation
â”œâ”€â”€ go.mod                      # Go module definition
â”œâ”€â”€ README.md                   # User guide
â””â”€â”€ REPORT.md                   # This document
```

### B. Configuration Reference

**File**: [`configs/config.yaml`](configs/config.yaml)

```yaml
master:
  host: "localhost"
  port: 5000

chunkserver:
  addresses:
    - ":6000"
    - ":6001"
    - ":6002"
    - ":6003"
  heartbeat_interval: 5  # seconds

chunk:
  size_bytes: 67108864    # 64 MB
  replication: 3

lease:
  duration: 60            # seconds
```

### C. Proto Definitions

**ChunkServer Service**: [`api/proto/chunkserver.proto`](api/proto/chunkserver.proto)

```protobuf
service ChunkServer {
  rpc PushData(stream PushDataRequest) returns (PushDataResponse) {}
  rpc AppendRecord(AppendRecordRequest) returns (AppendRecordResponse) {}
  rpc ForwardAppend(ForwardAppendRequest) returns (ForwardAppendResponse) {}
  rpc CheckIdempotency(IdempotencyStatusRequest) returns (IdempotencyStatusResponse) {}
  rpc ReadChunk(ReadChunkRequest) returns (ReadChunkResponse) {}
  rpc WriteChunk(WriteChunkRequest) returns (WriteChunkResponse) {}
  rpc ForwardWrite(ForwardWriteRequest) returns (ForwardWriteResponse) {}
}
```

**Master Service**: [`api/proto/master.proto`](api/proto/master.proto)

```protobuf
service Master {
  rpc GetFileChunksInfo(GetFileChunksInfoRequest) returns (GetFileChunksInfoResponse) {}
  rpc CreateFile(CreateFileRequest) returns (Status) {}
  rpc DeleteFile(DeleteFileRequest) returns (Status) {}
  rpc RenameFile(RenameFileRequest) returns (Status) {}
  rpc HeartBeat(HeartBeatRequest) returns (HeartBeatResponse) {}
  rpc RequestLease(RequestLeaseRequest) returns (RequestLeaseResponse) {}
  rpc AllocateChunk(AllocateChunkRequest) returns (AllocateChunkResponse) {}
  rpc ListFiles(ListFilesRequest) returns (ListFilesResponse) {}
}
```

### D. Build and Run Commands

**Build All**:
```bash
make proto  # Generate Go code from .proto files
go build ./cmd/master
go build ./cmd/chunkserver
go build ./cmd/client
```

**Start System**:
```powershell
# Terminal 1: Master
go run ./cmd/master

# Terminal 2-5: ChunkServers
go run ./cmd/chunkserver --addr :6000 --data data/cs0
go run ./cmd/chunkserver --addr :6001 --data data/cs1
go run ./cmd/chunkserver --addr :6002 --data data/cs2
go run ./cmd/chunkserver --addr :6003 --data data/cs3
```

**Client Operations**:
```powershell
# Create file
go run ./cmd/client --cmd create --remote /test.txt

# Append data
echo "Hello, GFS!" | Out-File test.txt
go run ./cmd/client --cmd append --remote /test.txt --input test.txt

# List files
go run ./cmd/client --cmd list
```

**Run Benchmarks**:
```powershell
cd benchmarks
go run ./throughput/throughput_bench.go -clients 4 -duration 30
go run ./latency/latency_bench.go -clients 2 -ops 200
```

### E. References

1. **Original GFS Paper**: [The Google File System (SOSP 2003)](https://research.google/pubs/pub51/)
2. **gRPC Documentation**: [grpc.io](https://grpc.io)
3. **Protocol Buffers**: [developers.google.com/protocol-buffers](https://developers.google.com/protocol-buffers)
4. **Two-Phase Commit**: [Wikipedia - 2PC](https://en.wikipedia.org/wiki/Two-phase_commit_protocol)

---

**End of Report**
